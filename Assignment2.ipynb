{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meezus510/ucsc/blob/master/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDo6Mz1MoJxR",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "Turn in the assignment via Canvas.\n",
        "\n",
        "To write legible answers you will need to be familiar with both [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) and [Latex](https://www.latex-tutorial.com/tutorials/amsmath/)\n",
        "\n",
        "Before you turn this problem in, make sure everything runs as expected. First, restart the kernel (in the menubar, select Kernel→→Restart) and then run all cells (in the menubar, select Cell→→Run All).\n",
        "\n",
        "Make sure you fill in any place that says \"YOUR CODE HERE\" or \"YOUR ANSWER HERE\", as well as your name below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOfazHDtoJxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NAME = \"\"\n",
        "STUDENT_ID = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyRMGyp9Y9c3",
        "colab_type": "text"
      },
      "source": [
        "## Question 1 - Linear Regression\n",
        "\n",
        "Inspiration for this question comes from: [1](https://mubaris.com/posts/linear-regression/) and [2](http://vxy10.github.io/2016/06/25/lin-reg-matrix/).\n",
        "\n",
        "In this question, you will be implementing the linear regression algorithm from scratch in Python. As you know, linear regression aims to map feature vectors to a continuous value in the range $[-\\infty,+\\infty]$ by linearly combining the feature values.\n",
        "\n",
        "### Model Representation\n",
        "As you have seen previously in assignment 1, we represent our data as a dataframe or a feature matrix. \n",
        "\n",
        "Let our feature matrix be $X$ whose dimensions are $n \\times m$, $\\theta$ be a weight matrix of dimensions $m \\times 1$, the bias vector $b$ a column vector of dimension $n\\times 1$. Using these we can predict $\\hat{Y}$\n",
        "by the following relationship:\n",
        "\n",
        "$$\\hat{Y} = X\\theta + b$$ \n",
        "\n",
        "(Does this look familiar? Remember $y = mx + b$)\n",
        "\n",
        "### Data: Facebook posts metrics\n",
        "\n",
        "This data contains features describing posts from a cosmetic brand's Facebook page. The authors use the following features: \n",
        "\n",
        "* Category,\n",
        "* Page total likes: Number of people who have liked the company's page), \n",
        "* Type: Type of content (Link, Photo, Status, Video), \n",
        "* Post month: Month the post was published (January, February, March, …, December), \n",
        "* Post hour: Hour the post was published (0, 1, 2, 3, 4, …, 23) , \n",
        "* Post weekday: Weekday the post was published (Sunday, Monday, …,\n",
        "Saturday) , \n",
        "* Paid: If the company paid to Facebook for advertising (yes, no)\n",
        "\n",
        "to model: \n",
        "\n",
        "'Lifetime Post Total Reach', 'Lifetime Post Total Impressions', 'Lifetime Engaged Users', 'Lifetime Post Consumers',\n",
        "'Lifetime Post Consumptions', 'Lifetime Post Impressions by people who have liked your Page', 'Lifetime Post reach by people who like your Page', 'Lifetime People who have liked your Page and engaged with your post', 'comment', 'like', 'share', 'Total Interactions'.\n",
        "\n",
        "\n",
        "There are many possible features we could try to model, but we will focus on 'Total Interactions'. Our feature space will include: Category, Page total likes, Post month, Post hour, Post weekday, and Paid. We drop \"Type\" simply to avoid preprocessing.\n",
        "\n",
        "You can read more about the dataset [here](http://archive.ics.uci.edu/ml/datasets/Facebook+metrics).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVmH8qFoc3pi",
        "colab_type": "text"
      },
      "source": [
        "### Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyeKP1Hqc4OU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/00368/Facebook_metrics.zip -O ./Facebook_metrics.zip\n",
        "import zipfile\n",
        "with zipfile.ZipFile('./Facebook_metrics.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbxl1mNMrxcQ",
        "colab_type": "text"
      },
      "source": [
        "### Reading in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm-P1EUzZCFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(144)\n",
        "'''\n",
        "Shuffles the data in place \n",
        "'''\n",
        "def shuffle_data(data):\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "# Read in the data\n",
        "lr_dataframe = pd.read_csv('dataset_Facebook.csv',sep=';')\n",
        "lr_dataframe.dropna(inplace=True)\n",
        "columns_to_drop = ['Type','Lifetime Post Total Reach', 'Lifetime Post Total Impressions',\n",
        "       'Lifetime Engaged Users', 'Lifetime Post Consumers',\n",
        "       'Lifetime Post Consumptions',\n",
        "       'Lifetime Post Impressions by people who have liked your Page',\n",
        "       'Lifetime Post reach by people who like your Page',\n",
        "       'Lifetime People who have liked your Page and engaged with your post',\n",
        "       'comment', 'like', 'share']\n",
        "lr_dataframe.drop(columns=columns_to_drop,inplace=True)\n",
        "\n",
        "# Normalizing all remaining columns\n",
        "def normalize_col(col):\n",
        "    return (col - col.min())/(col.max() - col.min())\n",
        "  \n",
        "lr_dataframe = lr_dataframe.apply(normalize_col)\n",
        "\n",
        "# Get entries as a numpy array\n",
        "lr_data = lr_dataframe.values[:, :]\n",
        "\n",
        "# Shuffle once for reproducibility\n",
        "shuffle_data(lr_data)\n",
        "\n",
        "lr_dataframe.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz0NSog8sH6i",
        "colab_type": "text"
      },
      "source": [
        "### a) Splitting data in $X$ and $Y$\n",
        "In this part we will write functions to split our data into a feature matrix X (augmented by a column of 1's to account for the bias term) and a column vector we wish to predict Y, shuffle our data, and finally further split X and Y into X_train, X_test, y_train and y_test for training and testing.\n",
        "\n",
        "i) Split the dataset into $X$ and $Y$. In order to vectorize our calculations, we utilize the [bias trick](https://cs231n.github.io/linear-classify/). This simply means we need to append ones to our $X$ matrix. \n",
        "\n",
        "ii) Shuffle whole dataset to randomize training and test data selection.\n",
        "\n",
        "ii) Split $X$ and $Y$ into the training and test sets using the provided percentage split (default is 80\\% training and 20\\% test)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7soUq_JAnog4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Combines one column of all ones and the matrix X to account for the bias term \n",
        "(setting x_0 = 1) - [Hint: you may want to use np.hstack()]\n",
        "Takes input matrix X\n",
        "Returns the augmented input \n",
        "'''\n",
        "def bias_trick(X):\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "'''\n",
        "Separates feature vectors and targets \n",
        "Takes raw data\n",
        "Returns X as the matrix of feature vectors and Y as the vector of targets \n",
        "'''\n",
        "def separate_data(data):\n",
        "    # Split into X (remember to use bias trick) and Y\n",
        "    # YOUR CODE HERE\n",
        "  \n",
        "'''\n",
        "Takes raw data in and splits the data into \n",
        "X_train, y_train, X_test, y_test\n",
        "Returns X_train, y_train, X_test, y_test \n",
        "'''\n",
        "def train_test_split(data, train_size=.80):\n",
        "    # YOUR CODE HERE\n",
        "    return "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6KAu3sxxzU_",
        "colab_type": "text"
      },
      "source": [
        "### b) Training and testing our model\n",
        "**Refer to the following derivation of the gradient when implementing linear regression and gradient descent below.**\n",
        "\n",
        "For this question, we'll use and implement the Mean Squared Error (MSE), and gradient descent algorithm. Suppose our dataset consists of $n$ records, each with $d$ features:\n",
        "\n",
        "$$X =\n",
        "\\begin{bmatrix}\n",
        "    x_{1,1}       & x_{1,2} & \\cdots & x_{1,d} \\\\\n",
        "    x_{2,1}       & x_{2,2} & \\cdots & x_{2,d} \\\\\n",
        "    \\vdots       & \\vdots & \\ddots & \\vdots \\\\\n",
        "    x_{n,1}       & x_{n,2} & \\cdots & x_{n,d}\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "One way to include a bias is to augment $X$ with a column of ones:\n",
        "\n",
        "$$X =\n",
        "\\begin{bmatrix}\n",
        "    1 & x_{1,1}       & x_{1,2} & \\cdots & x_{1,d} \\\\\n",
        "    1 & x_{2,1}       & x_{2,2} & \\cdots & x_{2,d} \\\\\n",
        "    \\vdots  & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    1 & x_{n,1}       & x_{n,2} & \\cdots & x_{n,d}\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "We also have $n$ labels corresponding to the correct classification of each of the above records, $y=[y_1,y_2,\\cdots ,y_{n}]^T$, i.e.:\n",
        "\n",
        "$$y =\n",
        "\\begin{bmatrix}\n",
        "    y_1  \\\\\n",
        "    y_2  \\\\\n",
        "    \\vdots   \\\\\n",
        "    y_{n} \n",
        "\\end{bmatrix}$$\n",
        "\n",
        "We will try to find the optimal parameter values $\\theta = [\\theta_0, \\theta_1, \\cdots, \\theta_d]^T$ of our linear regression model, where $\\theta_0$ is the bias weight. To simplify our notation, let\n",
        " \n",
        "$$\\hat{y} = X \\theta =\n",
        "\\begin{bmatrix}\n",
        "    X_{1,0}\\theta_0 + X_{1,1}\\theta_1 + \\cdots + X_{1,d}\\theta_d  \\\\\n",
        "    X_{2,0}\\theta_0 + X_{2,1}\\theta_1 + \\cdots + X_{2,d}\\theta_d  \\\\\n",
        "    \\vdots   \\\\\n",
        "    X_{n,0}\\theta_0 + X_{n,1}\\theta_1 + \\cdots + X_{n,d}\\theta_d \n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "    z_1  \\\\\n",
        "    z_2  \\\\\n",
        "    \\vdots   \\\\\n",
        "    z_{n}\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "We seek $\\theta$ such that the MSE is minimized (the 1/2 factor makes the derivation easier). Let the MSE be a function of $\\theta, J(\\theta)$:\n",
        "\n",
        "$$J(\\theta) = \\frac{1}{2n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2$$\n",
        "\n",
        "Since the above is a convex function, it has a unique minimum value. Taking the derivative with respect to $\\theta_i$, we get:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta) = \\frac{1}{2n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial\\theta_j}(\\hat{y}_i - y_i)^2$$\n",
        "\n",
        "$$\\quad \\quad \\quad \\quad= \\frac{1}{n}\\sum_{i=1}^{n} (\\hat{y}_i - y_i) \\frac{\\partial}{\\partial\\theta_j} (\\hat{y}_i)$$\n",
        "\n",
        "Recall the chain rule from calculus, and that each $\\hat{y}_i$ is a funcion of the $\\theta_i$, so the above becomes:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} ( \\hat{y_i} - y_i)x_{i,j}$$\n",
        "\n",
        "YOU SHOULD: \n",
        "\n",
        "i) Get training and testing set by calling train_test_split()\n",
        "\n",
        "ii) Define a weight ($\\theta$) vector\n",
        "\n",
        "ii) Implement Gradient Descent using the information above\n",
        "\n",
        "iii) Record the Sum Squared Error for training and test data\n",
        "\n",
        "iv) Return the weight matrix, train errors and test errors\n",
        "\n",
        "v) Plot the training and test errors and comment on the plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM8kWvu9qntV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Takes the target values and predicted values and calculates the absolute error \n",
        "between them \n",
        "'''\n",
        "def mse(y_pred, y_true):\n",
        "    # YOUR CODE HERE\n",
        "    return\n",
        "\n",
        "'''\n",
        "Implementation of the derivative of MSE.\n",
        "Returns a vecotr of derivations of loss with respect to each of the dimensions\n",
        "[\\partial loss / \\partial \\theta_i]\n",
        "'''\n",
        "def mse_derivative(X,y,theta):\n",
        "    # YOUR CODE HERE\n",
        "    return\n",
        "\n",
        "'''\n",
        "Gradient descent step. \n",
        "Takes X, y, theta vector, and alpha. \n",
        "Returns an updated theta vector.\n",
        "'''\n",
        "def gradient_descent_step(X,y, theta, alpha):\n",
        "    # YOUR CODE HERE\n",
        "    return theta\n",
        "\n",
        "\n",
        "def linear_regression(data, num_epochs=3250, alpha=0.00005):\n",
        "    # Get training and testing set by calling train_test_split()\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # Record training and test errors in lists\n",
        "    train_errors= []\n",
        "    test_errors= []\n",
        "\n",
        "    # Define theta\n",
        "    theta = np.zeros((X_train.shape[1]))\n",
        "\n",
        "    # Carry out training loop\n",
        "    for i in range(num_epochs):\n",
        "        train_error = # YOUR CODE HERE\n",
        "        train_errors.append(train_error)\n",
        "\n",
        "        test_error = # YOUR CODE HERE\n",
        "        test_errors.append(test_error)\n",
        "\n",
        "        # Do gradient descent on the training set\n",
        "        theta = # YOUR CODE HERE\n",
        "    return theta, train_errors, test_errors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu6NeHgMudA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Carry out training task\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Plot the training error and test error for different epochs (iterations of the\n",
        "# algorithm). Your plot be MSE error vs epochs.\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCDbpvaZ3xN4",
        "colab_type": "text"
      },
      "source": [
        "### c) How did we do?\n",
        "\n",
        "Please comment on the performance of the model you trained on training and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5xG45OV3mTp",
        "colab_type": "text"
      },
      "source": [
        "#### YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_LtDwYAov6R",
        "colab_type": "text"
      },
      "source": [
        "## Question 2\n",
        "---\n",
        "### Fitting non-linear data\n",
        "\n",
        "Data may not follow a linear relationship from the independent variable $X$ to the dependent variable $y$. Fitting a linear model to this would be inaccurate and yield a high loss. \n",
        "\n",
        "If we want to model an order $d$ polynomial relationship between $X$ and $y$ we can augment our initial linear model where instead of having:\n",
        "$$\n",
        "y_i = \\theta_0 + \\theta_1 x_i\n",
        "$$\n",
        "\n",
        "We have:\n",
        "\n",
        "$$\n",
        "y_i = \\theta_0 + \\theta_1 x_i + \\theta_2 x_i^2 + \\cdots + \\theta_d x_i^d\n",
        "$$\n",
        "\n",
        "We can use the same linear regression algorithm we if we first augment $X$ and add extra columns (or dimensions). \n",
        "\n",
        "$$ \\textbf X =\n",
        "\\begin{bmatrix}\n",
        "    x_{1}       & x_{1}^2 & \\cdots & x_{1}^d \\\\\n",
        "    x_{2}       & x_{2}^2 & \\cdots & x_{2}^d \\\\\n",
        "    \\vdots       & \\vdots & \\ddots & \\vdots \\\\\n",
        "    x_{n}       & x_{n}^2 & \\cdots & x_{n}^d\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Then our new higher order $\\hat y$ is computed same as before.\n",
        "\n",
        "$$ \\hat y =  X \\theta =\n",
        "\\begin{bmatrix}\n",
        "    1 & x_{1}       & x_{1}^2 & \\cdots & x_{1}^d \\\\\n",
        "    1 & x_{2}       & x_{2}^2 & \\cdots & x_{2}^d \\\\\n",
        "    \\vdots & \\vdots       & \\vdots & \\ddots & \\vdots \\\\\n",
        "    1 & x_{n}       & x_{n}^2 & \\cdots & x_{n}^d\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_{d} \n",
        "\\end{bmatrix}=\n",
        "\\begin{bmatrix}\n",
        "    \\theta_0 + \\theta_1 x_{1} + \\theta_2 x_{1}^2 + \\cdots + \\theta_{d}  x_{1}^d \\\\\n",
        "    \\theta_0 + \\theta_1 x_{2} + \\theta_2 x_{2}^2 + \\cdots + \\theta_{d}  x_{2}^d  \\\\\n",
        "    \\vdots   \\\\\n",
        "    \\theta_0 + \\theta_1 x_{n} + \\theta_2 x_{n}^2 + \\cdots + \\theta_{d}  x_{n}^d\n",
        "\\end{bmatrix} \n",
        "= \\begin{bmatrix}\\hat y_1 \\\\ \\hat y_2 \\\\ \\vdots \\\\ \\hat y_{n} \n",
        "\\end{bmatrix}$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8Y03UHppBB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def normalize_data(data):\n",
        "    return (data - np.min(data))/(np.max(data) - np.min(data))\n",
        "\n",
        "np.random.seed(33)\n",
        "x = np.random.uniform(-10, 10, 1000)\n",
        "poly_coeffs = np.random.uniform(-1,1, size=(4,1))\n",
        "y = poly_coeffs[0] + poly_coeffs[1]*x + poly_coeffs[2]*(x ** 2) + poly_coeffs[3]*(x ** 3) + np.random.normal(0, 250, 1000)\n",
        "\n",
        "x2 = np.random.uniform(-10, 10, 1000)\n",
        "poly_coeffs = np.random.uniform(-1,1, size=(3,1))\n",
        "y2 = poly_coeffs[0] - 2000 + poly_coeffs[1]*x2 + 50*poly_coeffs[2]*(x2 ** 2)  + np.random.normal(0, 250, 1000)\n",
        "\n",
        "x = np.concatenate([x,x2])\n",
        "y = np.concatenate([y,y2])\n",
        "x = normalize_data(x)\n",
        "y = normalize_data(y)\n",
        "\n",
        "plt.scatter(x,y, s=10)\n",
        "plt.show()\n",
        "\n",
        "poly_data = np.hstack((x.reshape(-1,1),y.reshape(-1,1)))\n",
        "np.random.shuffle(poly_data)\n",
        "x = poly_data[:,0]\n",
        "y = poly_data[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8wyk6w5TwVH8",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkMVMvK4wWW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reg = LinearRegression().fit(x.reshape(-1,1), y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDrArx_rwWcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_line_from_regr(X_data, y_data, regr):\n",
        "    l_bound = np.min(X_data)\n",
        "    r_bound = np.max(X_data)\n",
        "    return [l_bound, r_bound], [l_bound * regr.coef_ + regr.intercept_, r_bound * regr.coef_ + regr.intercept_]\n",
        "\n",
        "plt.scatter(x,y, s=10)\n",
        "line_x, line_y = compute_line_from_regr(x.reshape(-1,1),y,reg)\n",
        "plt.plot(line_x, line_y, color='r')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crXLnJ2h7tBx",
        "colab_type": "text"
      },
      "source": [
        "As we see above, this data doesn't follow a linear relationship, it follows some complex polynomial. In the next section you'll try to fit a higher degree polynomial to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2cxVyPq7Ua6",
        "colab_type": "text"
      },
      "source": [
        "## Weight regularization\n",
        "When we try to fit a d-order polynomial to our data, we could end up overfitting. This happens when you try to fit a higher dimensional curve than what the distribution of our data actually exhibits. We can mitigate this by choosing an order $d$ that matches your data closely, but often times this is not directly apperant in noisy data. Another method to avoid overfitting is **regularizing**, where you modify your loss to keep weights small which flattens our polynomial. This helps us avoid learning polynomials that are too complex for our data.\n",
        "\n",
        "To add regularization we modify our original loss function $J$ to include our regularizing term and a new hyperparameter that we tune $\\lambda$. This controls the amount of regularizing we impose on the weights. We use the loss computed from the validation set to tweak this parameter.\n",
        "\n",
        "$$\n",
        "J(\\theta)=\\frac{1}{2n}\\sum^{n}_{i=1}(h^{(i)}-y^{(i)})^2 + \\lambda \\sum^{d}_{j=1} \\theta^2_j\n",
        "$$\n",
        "\n",
        "Our gradient computation also changes:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial\\theta_j}J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} ( h^{(i)}-y^{(i)})x_{i,j}  + 2 \\lambda\\theta_j \n",
        "$$\n",
        "\n",
        "We apply this gradient the same way as before in our gradient descent algorithm:\n",
        "$$\n",
        " \\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j}J(\\theta)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-d6NbmBJL1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Takes raw data in and splits the data into \n",
        "X_train, y_train, X_test, y_test, X_val, y_val\n",
        "Returns X_train, y_train, X_test, y_test, X_val, y_val\n",
        "'''\n",
        "def train_test_validation_split(data, test_size=.20, validation_size=.20):\n",
        "    # YOUR CODE HERE\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc4wvXybwWfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Adds columns to your data up to the specified degree.\n",
        "Ex: If degree=3, (x) -> (x, x^2, x^3)\n",
        "'''\n",
        "def add_polycols(X,degree):\n",
        "    x_col = X[:,-1]\n",
        "\n",
        "    for i in range(2, degree+1):\n",
        "        X = np.hstack((X,(x_col**i).reshape(-1,1)))\n",
        "    return X\n",
        "\n",
        "'''\n",
        "Takes the target values and predicted values and calculates the absolute error \n",
        "between them \n",
        "'''\n",
        "def mse(y_pred, y_true):\n",
        "    # YOUR CODE HERE\n",
        "    # Feel free to use your implementation from Q1  \n",
        "    return\n",
        "\n",
        "'''\n",
        "Implementation of the derivative of MSE.\n",
        "Returns a vector of derivations of loss with respect to each of the dimensions\n",
        "[\\partial loss / \\partial \\theta_i]\n",
        "'''\n",
        "def mse_derivative(X,y,theta):\n",
        "    # YOUR CODE HERE\n",
        "    # Feel free to use your implementation from Q1\n",
        "    return\n",
        "\n",
        "'''\n",
        "Computes L2 norm from theta and lambda.\n",
        "Returns a vector of L2 norms.\n",
        "'''\n",
        "def l2norm(theta, lamb):\n",
        "    # YOUR CODE HERE\n",
        "    return\n",
        "\n",
        "'''\n",
        "Computes derivative of L2 norm.\n",
        "Returns a vector of derivative of L2 norms.\n",
        "'''\n",
        "def l2norm_derivative(theta, lamb):\n",
        "    # YOUR CODE HERE\n",
        "    # Note there is no regularization on the bias term.\n",
        "    return\n",
        "\n",
        "'''\n",
        "Computes total cost (cost function + regularization term)\n",
        "'''\n",
        "def compute_cost(X, y, theta, lamb):\n",
        "    # YOUR CODE HERE\n",
        "    return\n",
        "\n",
        "'''\n",
        "Gradient descent step. \n",
        "Takes X, y, theta vector, and alpha. \n",
        "Returns an updated theta vector.\n",
        "'''\n",
        "def gradient_descent_step(X, y, theta, alpha, lamb):\n",
        "    # YOUR CODE HERE\n",
        "    # This differs from your Q1 implementation\n",
        "    return\n",
        "\n",
        "\n",
        "def polynomial_regression(data, degree, num_epochs=100000, alpha=1e-4, lamb=0):\n",
        "    # Get training, testing, and validation sets by calling train_test_validation_split()\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # Record training and validation errors in lists\n",
        "    train_errors = []\n",
        "    val_errors = []\n",
        "\n",
        "    # Add the appropriate amount of columns to each of your sets of data.\n",
        "    X_train = add_polycols(X_train, degree)\n",
        "    X_val = add_polycols(X_val, degree)\n",
        "    X_test = add_polycols(X_test, degree)\n",
        "\n",
        "    # Define theta\n",
        "    theta = np.zeros((X_train.shape[1]))\n",
        "    # Carry out training loop\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "        train_error = # YOUR CODE HERE\n",
        "        train_errors.append(train_error)\n",
        "\n",
        "        val_error = # YOUR CODE HERE\n",
        "        val_errors.append(val_error)\n",
        "        \n",
        "        # Do gradient descent on the training set\n",
        "        theta = # YOUR CODE HERE\n",
        "\n",
        "        # This prints the validation loss\n",
        "        if i % (num_epochs//10) == 0:\n",
        "            print(f'({i} epochs) Training loss: {train_error}, Validation loss: {val_error}') \n",
        "    print(f'({i} epochs) Final training loss: {train_error}, Final validation loss: {val_error}') \n",
        "    \n",
        "    # Compute the testing loss\n",
        "    test_error = # YOUR CODE HERE\n",
        "    print(f'Final testing loss: {test_error}')\n",
        "    return theta, train_errors, val_errors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmyCKPqU8Qcw",
        "colab_type": "text"
      },
      "source": [
        "As we mentioned above, we use the validation set's loss to tweak our hyperparameters. Please carry out the training task while monitoring the validation loss and varying the polynomial order $d$ and regularization constant $\\lambda$. Your answer should get close to minimizing the validation and testing losses. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GQeUM84wWai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# degree d\n",
        "polynomial_order = \n",
        "\n",
        "# regularization constant lambda\n",
        "regularization_param = \n",
        "\n",
        "theta, train_errors, val_errors = polynomial_regression(poly_data, polynomial_order, lamb=regularization_param, num_epochs=100000, alpha=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-oAo20T3Tph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Call plot_results() to see how your polynomial fits.\n",
        "def plot_results(theta, X, Y):\n",
        "    y_hat = sum([t*X**i for i,t in enumerate(theta)])\n",
        "    plt.scatter(X, y_hat, s=10, color='r')\n",
        "    plt.scatter(X, Y, s=10)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}